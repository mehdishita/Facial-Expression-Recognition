{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "R4VcTUFOctjj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structuré enregistré dans train_labeled_dataset.csv\n",
      "Données exemple\n",
      "                               image_path  label\n",
      "0  data/train\\angry\\Training_10118481.jpg  angry\n",
      "1  data/train\\angry\\Training_10120469.jpg  angry\n",
      "2  data/train\\angry\\Training_10131352.jpg  angry\n",
      "3  data/train\\angry\\Training_10161559.jpg  angry\n",
      "4   data/train\\angry\\Training_1021836.jpg  angry\n",
      "5  data/train\\angry\\Training_10269675.jpg  angry\n",
      "6  data/train\\angry\\Training_10278738.jpg  angry\n",
      "7  data/train\\angry\\Training_10290703.jpg  angry\n",
      "8  data/train\\angry\\Training_10295477.jpg  angry\n",
      "9  data/train\\angry\\Training_10315441.jpg  angry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin du dossier principal contenant les sous-dossiers\n",
    "dataset_dir = \"data/train\"\n",
    "\n",
    "# Liste pour stocker les informations\n",
    "data = []\n",
    "\n",
    "# Parcourir chaque sous-dossier\n",
    "for label in os.listdir(dataset_dir):\n",
    "    subfolder_path = os.path.join(dataset_dir, label)\n",
    "    \n",
    "    # Vérifier si c'est bien un dossier\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Parcourir les fichiers dans le sous-dossier\n",
    "        for file_name in os.listdir(subfolder_path):\n",
    "            file_path = os.path.join(subfolder_path, file_name)\n",
    "            \n",
    "            # Vérifier si c'est bien une image\n",
    "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                data.append({\"image_path\": file_path, \"label\": label})\n",
    "\n",
    "# Créer un DataFrame Pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "output_csv = \"train_labeled_dataset.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Dataset structuré enregistré dans {output_csv}\")\n",
    "print(\"Données exemple\")\n",
    "print(df[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structuré enregistré dans test_labeled_dataset.csv\n",
      "Données exemple\n",
      "                                 image_path  label\n",
      "0  data/test\\angry\\PrivateTest_10131363.jpg  angry\n",
      "1  data/test\\angry\\PrivateTest_10304478.jpg  angry\n",
      "2   data/test\\angry\\PrivateTest_1054527.jpg  angry\n",
      "3  data/test\\angry\\PrivateTest_10590091.jpg  angry\n",
      "4   data/test\\angry\\PrivateTest_1109992.jpg  angry\n",
      "5  data/test\\angry\\PrivateTest_11296953.jpg  angry\n",
      "6  data/test\\angry\\PrivateTest_12000629.jpg  angry\n",
      "7  data/test\\angry\\PrivateTest_12008383.jpg  angry\n",
      "8  data/test\\angry\\PrivateTest_12191716.jpg  angry\n",
      "9   data/test\\angry\\PrivateTest_1221822.jpg  angry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin du dossier principal contenant les sous-dossiers\n",
    "dataset_dir = \"data/test\"\n",
    "\n",
    "# Liste pour stocker les informations\n",
    "data = []\n",
    "\n",
    "# Parcourir chaque sous-dossier\n",
    "for label in os.listdir(dataset_dir):\n",
    "    subfolder_path = os.path.join(dataset_dir, label)\n",
    "    \n",
    "    # Vérifier si c'est bien un dossier\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Parcourir les fichiers dans le sous-dossier\n",
    "        for file_name in os.listdir(subfolder_path):\n",
    "            file_path = os.path.join(subfolder_path, file_name)\n",
    "            \n",
    "            # Vérifier si c'est bien une image\n",
    "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                data.append({\"image_path\": file_path, \"label\": label})\n",
    "\n",
    "# Créer un DataFrame Pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sauvegarder dans un fichier CSV\n",
    "output_csv = \"test_labeled_dataset.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Dataset structuré enregistré dans {output_csv}\")\n",
    "print(\"Données exemple\")\n",
    "print(df[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I29t_i5NdKOf"
   },
   "source": [
    "#1- Normalisation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2aIcpUyUhpaq"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall dlib\n",
    "#!pip install dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OqC9uFXmcy_u"
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat')\n",
    "\n",
    "def face_locations(image, model=\"hog\"):\n",
    "\n",
    "\n",
    "    detector = hog_detector\n",
    "    cst = 0\n",
    "    #elif model == \"cnn\":\n",
    "    #    detector = cnn_detector\n",
    "    #    cst = 10\n",
    "\n",
    "    matches = detector(image,1)\n",
    "    rects   = []\n",
    "\n",
    "    for r in matches:\n",
    "        if model == \"cnn\":\n",
    "            r = r.rect\n",
    "        x = max(r.left(), 0)\n",
    "        y = max(r.top(), 0)\n",
    "        w = min(r.right(), image.shape[1]) - x + cst\n",
    "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
    "        rects.append((x,y,w,h))\n",
    "\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6rzZHz8dc1Uc"
   },
   "outputs": [],
   "source": [
    "def extract_faces(image, model=\"hog\"):\n",
    "\n",
    "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = face_locations(gray, model)\n",
    "    faces = []\n",
    "\n",
    "    for (x,y,w,h) in rects:\n",
    "        cropped = image[y:y+h, x:x+w, :]\n",
    "        cropped = cv2.resize(cropped, (128,128))\n",
    "        faces.append(cropped)\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "35V1Ga2YuShP",
    "outputId": "596e6f09-03da-4ed4-d896-37bfd2e359d4"
   },
   "source": [
    "image = cv2.imread(\"/content/drive/MyDrive/Projet Vision Numérique/archive.zip (Unzipped Files)/Train provisoire/Training_99746769.jpg\")\n",
    "image1 = cv2.imread(\"/content/drive/MyDrive/Projet Vision Numérique/archive.zip (Unzipped Files)/Train provisoire/Training_99746769.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuizKSO_ukko",
    "outputId": "734aa12e-d858-41a3-ea65-d89297d96843"
   },
   "source": [
    "location = face_locations(image)\n",
    "print(location)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pe1JbhRQwWur",
    "outputId": "234d57d4-b676-40c7-b273-9c9e9763dd76"
   },
   "source": [
    "faces = extract_faces(image)\n",
    "print(len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hJIDhoOwVDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CtL_sNsmc4rg"
   },
   "outputs": [],
   "source": [
    "def show_grid(faces, figsize=(8,8)):\n",
    "    face = faces[0]\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(face)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "id": "r1WDZLz5wh9v",
    "outputId": "cebfe25c-65ce-4aac-e82c-25a5ce301d62"
   },
   "source": [
    "show_grid(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8mXD9SpOc7fS"
   },
   "outputs": [],
   "source": [
    "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
    "\n",
    "    imagePaths = []\n",
    "\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
    "                imagePaths.append(imagePath)\n",
    "\n",
    "    return imagePaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16XdmhM9dOel"
   },
   "source": [
    "# 2- Estimation du visage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
    "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EUbRu7ykdY28"
   },
   "outputs": [],
   "source": [
    "def face_landmarks(face, model=\"large\"):\n",
    "\n",
    "    if model == \"large\":\n",
    "        predictor = pose68\n",
    "    elif model == \"small\":\n",
    "        predictor = pose05\n",
    "\n",
    "    if not isinstance(face, list):\n",
    "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
    "        return predictor(face, rect)\n",
    "    else:\n",
    "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
    "        return [predictor(f,rect) for f in face]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "Zx3cK5tcxI-q"
   },
   "source": [
    "marks = face_landmarks(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "O1LJNBcsdZzR"
   },
   "outputs": [],
   "source": [
    "def shape_to_coords(shape):\n",
    "    return np.float32([[p.x, p.y] for p in shape.parts()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ipr4j3FXdzkL"
   },
   "outputs": [],
   "source": [
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
    "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wc5-XyGwd2k9"
   },
   "outputs": [],
   "source": [
    "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
    "    faces = []\n",
    "    for (img, marks) in zip(images, landmarks):\n",
    "        imgDim = img.shape[0]\n",
    "        coords = shape_to_coords(marks)\n",
    "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
    "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
    "        faces.append(warped)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "nFPRPH0BieHj",
    "outputId": "b16c0bf5-e16e-4f99-9819-59947361a48e"
   },
   "outputs": [],
   "source": [
    "#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "_JRovgoreAPr"
   },
   "source": [
    "landmarks = face_landmarks(faces)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "kwC-kYa6eEwn"
   },
   "source": [
    "new_faces = []\n",
    "for (face,shape) in zip(faces, landmarks):\n",
    "    canvas = face.copy()\n",
    "    coords = shape_to_coords(shape)\n",
    "    for p in coords:\n",
    "        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n",
    "    new_faces.append(canvas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "UPPy6dDGeHOF",
    "outputId": "048fb3f1-2014-4ed2-990e-ac9d9823dd34"
   },
   "source": [
    "show_grid(new_faces, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "huq00rU3eKQ4"
   },
   "source": [
    "aligned = align_faces(faces, landmarks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "fOhThe2XeWN1",
    "outputId": "c155304c-83c5-4c8d-d6e7-122ed0a52bd4"
   },
   "source": [
    "show_grid(aligned, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "w29nOXSyzTFg"
   },
   "source": [
    "landmarks = face_landmarks(aligned)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "BqVre46HzXVb"
   },
   "source": [
    "new_faces = []\n",
    "for (face,shape) in zip(aligned, landmarks):\n",
    "    canvas = face.copy()\n",
    "    coords = shape_to_coords(shape)\n",
    "    for p in coords:\n",
    "        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n",
    "    new_faces.append(canvas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "wg51VXX_zY4S",
    "outputId": "32471063-3d90-4641-f273-b9defc918d9b"
   },
   "source": [
    "show_grid(new_faces, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images_dir = \"processed_images\"\n",
    "def process_and_save_image(row, model=\"hog\"):\n",
    "    image_path = row[\"image_path\"]\n",
    "    label = row[\"label\"]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Détecter et extraire les visages\n",
    "    faces = extract_faces(image, model)\n",
    "\n",
    "    if not faces:\n",
    "        return None  # Aucun visage détecté\n",
    "\n",
    "    # Extraire les landmarks et aligner\n",
    "    landmarks = [face_landmarks(face) for face in faces]\n",
    "    aligned_faces = align_faces(faces, landmarks)\n",
    "\n",
    "    # Sauvegarder les images alignées\n",
    "    saved_paths = []\n",
    "    for i, face in enumerate(aligned_faces):\n",
    "        processed_path = os.path.join(\n",
    "            processed_images_dir, f\"{label}_{os.path.basename(image_path)}_{i}.jpg\"\n",
    "        )\n",
    "        cv2.imwrite(processed_path, cv2.cvtColor(face, cv2.COLOR_RGB2BGR))\n",
    "        saved_paths.append(processed_path)\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "def process_dataset(csv_path, output_csv, model=\"hog\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    processed_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        processed_paths = process_and_save_image(row, model)\n",
    "        if processed_paths:\n",
    "            for path in processed_paths:\n",
    "                processed_data.append({\"image_path\": path, \"label\": row[\"label\"]})\n",
    "\n",
    "    # Sauvegarder le nouveau DataFrame\n",
    "    processed_df = pd.DataFrame(processed_data)\n",
    "    processed_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed dataset saved to {output_csv}\")\n",
    "    return processed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to processed_train_labeled_dataset.csv\n",
      "Processed dataset saved to processed_test_labeled_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>processed_images\\angry_PrivateTest_1054527.jpg...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_images\\angry_PrivateTest_1109992.jpg...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_images\\angry_PrivateTest_11296953.jp...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_images\\angry_PrivateTest_12191716.jp...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>processed_images\\angry_PrivateTest_1221822.jpg...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>processed_images\\sad_PrivateTest_91626978.jpg_...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>processed_images\\sad_PrivateTest_92077958.jpg_...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>processed_images\\sad_PrivateTest_92593957.jpg_...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3968</th>\n",
       "      <td>processed_images\\sad_PrivateTest_92794448.jpg_...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>processed_images\\sad_PrivateTest_92827143.jpg_...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3970 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  label\n",
       "0     processed_images\\angry_PrivateTest_1054527.jpg...  angry\n",
       "1     processed_images\\angry_PrivateTest_1109992.jpg...  angry\n",
       "2     processed_images\\angry_PrivateTest_11296953.jp...  angry\n",
       "3     processed_images\\angry_PrivateTest_12191716.jp...  angry\n",
       "4     processed_images\\angry_PrivateTest_1221822.jpg...  angry\n",
       "...                                                 ...    ...\n",
       "3965  processed_images\\sad_PrivateTest_91626978.jpg_...    sad\n",
       "3966  processed_images\\sad_PrivateTest_92077958.jpg_...    sad\n",
       "3967  processed_images\\sad_PrivateTest_92593957.jpg_...    sad\n",
       "3968  processed_images\\sad_PrivateTest_92794448.jpg_...    sad\n",
       "3969  processed_images\\sad_PrivateTest_92827143.jpg_...    sad\n",
       "\n",
       "[3970 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traitez les datasets\n",
    "train_csv = \"train_labeled_dataset.csv\"\n",
    "test_csv = \"test_labeled_dataset.csv\"\n",
    "processed_train_csv = \"processed_train_labeled_dataset.csv\"\n",
    "processed_test_csv = \"processed_test_labeled_dataset.csv\"\n",
    "\n",
    "process_dataset(train_csv, processed_train_csv)\n",
    "process_dataset(test_csv, processed_test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCh-c83hZTQv"
   },
   "source": [
    "# 3 - Encodement du visage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFX_gtwgoW5v",
    "outputId": "1cb6f4a1-324a-4e7f-dc50-4129087268c9"
   },
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "GJrJ7SFPoQ4D",
    "outputId": "62bf281f-b258-4a0f-a94e-1f69f9f54198"
   },
   "outputs": [],
   "source": [
    "import dlib\n",
    "cnn_encoder = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permet d'extraire les coordonnées des marquages du visage\n",
    "def face_encoder(faces):\n",
    "\n",
    "    landmarks = face_landmarks(faces)\n",
    "\n",
    "    if not isinstance(faces, list):\n",
    "        return np.array(cnn_encoder.compute_face_descriptor(faces,landmarks))\n",
    "    else:\n",
    "        return np.array([cnn_encoder.compute_face_descriptor(f,l) for f,l in zip(faces,landmarks)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "__________________________\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install facenet-pytorch fer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.2+cpu\n",
      "Torchvision version: 0.17.2+cpu\n",
      "FER and facenet-pytorch modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import moviepy\n",
    "import fer\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(\"FER and facenet-pytorch modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = fer.FER(mtcnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_csv = \"train_labeled_dataset.csv\"\n",
    "df = pd.read_csv(processed_train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_vectors = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image_path = row['image_path']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Détecter les émotions\n",
    "    emotion_data = detector.detect_emotions(image)\n",
    "    \n",
    "    # Si des émotions sont détectées, stocker le vecteur\n",
    "    if emotion_data:\n",
    "        emotions = emotion_data[0]['emotions']  # Premier visage détecté\n",
    "        emotion_vectors.append(list(emotions.values()))\n",
    "        labels.append(label)\n",
    "\n",
    "X = np.array(emotion_vectors)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmtxR1LZaUIv"
   },
   "source": [
    "# 4 - Préparation du modèle d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils, layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sauvegarde dans un fichier .npy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_vectors.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, X)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Sauvegarde dans un fichier .csv (facile à lire)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_vectors.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, X, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Sauvegarde dans un fichier .npy\n",
    "np.save('emotion_vectors.npy', X)\n",
    "\n",
    "# Sauvegarde dans un fichier .csv (facile à lire)\n",
    "np.savetxt('emotion_vectors.csv', X, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger depuis le fichier .npy\n",
    "X = np.load('emotion_vectors.npy')\n",
    "\n",
    "# Charger depuis le fichier .csv\n",
    "X = np.loadtxt('emotion_vectors.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Encoder les labels\u001b[39;00m\n\u001b[0;32m      4\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m----> 5\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y)  \u001b[38;5;66;03m# Conversion en labels numériques\u001b[39;00m\n\u001b[0;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_classes.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, label_encoder\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoder les labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Conversion en labels numériques\n",
    "\n",
    "np.save('label_classes.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved classes\n",
    "classes = np.load('label_classes.npy')\n",
    "\n",
    "# Reinitialize the LabelEncoder and set its classes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Diviser en ensembles d'entraînement et de test\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y_encoded, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.38      0.41       668\n",
      "           1       0.65      0.22      0.33        90\n",
      "           2       0.27      0.20      0.23       655\n",
      "           3       0.68      0.77      0.72      1234\n",
      "           4       0.40      0.49      0.44       871\n",
      "           5       0.34      0.33      0.33       734\n",
      "           6       0.63      0.60      0.62       579\n",
      "\n",
      "    accuracy                           0.49      4831\n",
      "   macro avg       0.49      0.43      0.44      4831\n",
      "weighted avg       0.48      0.49      0.48      4831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialisation du model + entrainement KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')\n",
    "# GridSearchCV avec cross-validation\n",
    "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Entraînement\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "prediction_knn = grid_search_knn.predict(X_test)\n",
    "print(sklearn.metrics.classification_report(y_test, prediction_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a été enregistré sous le nom 'grid_search_knn.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Enregistrer le modèle GridSearchCV\n",
    "joblib.dump(grid_search_knn, 'grid_search_knn.pkl')\n",
    "print(\"Le modèle a été enregistré sous le nom 'grid_search_knn.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle KNN a été chargé avec succès\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe modèle KNN a été chargé avec succès\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Faire des prédictions sur X_test\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m loaded_predictions \u001b[38;5;241m=\u001b[39m grid_search_knn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Afficher le rapport de classification\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, loaded_predictions))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "# Charger le modèle enregistré\n",
    "grid_search_knn = joblib.load('grid_search_knn.pkl')\n",
    "print(\"Le modèle KNN a été chargé avec succès\")\n",
    "\n",
    "# Faire des prédictions sur X_test\n",
    "loaded_predictions = grid_search_knn.predict(X_test)\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(classification_report(y_test, loaded_predictions))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initialisation du model SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid_svc = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "\n",
    "grid_search_svc = GridSearchCV(svc_model, param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search_svc.fit(X_train, y_train)\n",
    "\n",
    "predictions_svm = grid_search_svc.predict(X_test)\n",
    "print(sklearn.metrics.classification_report(y_test, predictions_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Paramètres pour le SVC\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],  # Pénalité de l'erreur\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Types de noyaux\n",
    "    'degree': [3, 4, 5],  # Utilisé si kernel=poly\n",
    "    'gamma': ['scale', 'auto']  # Paramètre de la fonction noyau\n",
    "}\n",
    "\n",
    "# Initialisation du modèle SVC\n",
    "svc_model = SVC(gamma='auto')\n",
    "\n",
    "# GridSearchCV avec validation croisée\n",
    "grid_search_svc = GridSearchCV(svc_model, param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Entraînement du modèle\n",
    "grid_search_svc.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur les données de test\n",
    "predictions_svm = grid_search_svc.predict(X_test)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(classification_report(y_test, predictions_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Sauvegarder le modèle GridSearchCV entraîné\n",
    "joblib.dump(grid_search_svc, 'grid_search_svc_model.pkl')\n",
    "print(\"Modèle sauvegardé sous le nom 'grid_search_svc_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'grid_search_svc.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Charger le modèle enregistré\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m grid_search_svc \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_search_svc.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe modèle a été chargé avec succès\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Faire des prédictions sur X_test\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'grid_search_svc.pkl'"
     ]
    }
   ],
   "source": [
    "# Charger le modèle enregistré\n",
    "import joblib\n",
    "grid_search_svc = joblib.load('grid_search_svc.pkl')\n",
    "print(\"Le modèle a été chargé avec succès\")\n",
    "\n",
    "# Faire des prédictions sur X_test\n",
    "loaded_predictions = grid_search_svc.predict(X_test)\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "print(classification_report(y_test, loaded_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b' , label = 'Training loss')\n",
    "plt.plot(val_loss, 'm' , label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b' , label = 'Training acc')\n",
    "plt.plot(val_acc, 'm' , label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY3E-LYxacyu"
   },
   "source": [
    "# 5 - Processus sur une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(image, mode=\"fast\"):\n",
    "\n",
    "    # face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if mode == \"fast\":\n",
    "        matches = hog_detector(gray,1)\n",
    "    else:\n",
    "        matches = cnn_detector(gray,1)\n",
    "        matches = [m.rect for m in matches]\n",
    "\n",
    "    for rect in matches:\n",
    "\n",
    "        # face landmarks\n",
    "        landmarks = pose68(gray, rect)\n",
    "\n",
    "        # face encoding\n",
    "        encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
    "\n",
    "        # face classification\n",
    "        label = \"label\"\n",
    "\n",
    "        # draw box\n",
    "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
    "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wRJxWEfatsK"
   },
   "source": [
    "# 6 - Processus sur une vidéo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ckh6cv9gaj58"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_movie(video_path, model, face_cascade, emotion_to_emoji):\n",
    "    \"\"\"\n",
    "    Traite une vidéo pour prédire les émotions sur chaque frame et les afficher.\n",
    "    \"\"\"\n",
    "    # Charger la vidéo\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Détection des visages\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Prédiction d'émotions et affichage\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "            resized_face = cv2.resize(face, (128, 128))  # Redimensionner pour le modèle\n",
    "            face_features = extract_face_features(resized_face)  # Fonction d'extraction des caractéristiques\n",
    "            emotion = model.predict([face_features])\n",
    "\n",
    "            # Convertir l'émotion en avatar\n",
    "            emoji_image = emotion_to_avatar(emotion[0], emotion_to_emoji)\n",
    "            if emoji_image is not None:\n",
    "                # Positionner l'emoji sur le visage détecté\n",
    "                emoji_resized = cv2.resize(emoji_image, (50, 50))  # Ajuster la taille de l'emoji\n",
    "                frame[y:y + emoji_resized.shape[0], x + w + 10:x + w + 60] = emoji_resized\n",
    "\n",
    "            # Afficher le cadre du visage et l'émotion\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, emotion[0], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Afficher la vidéo avec les prédictions d'émotions\n",
    "        cv2.imshow(\"Emotion Prediction in Movie\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Emotions to avatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Recharger les classes sauvegardées\n",
    "label_classes = np.load('label_classes.npy')\n",
    "\n",
    "# Initialiser un nouveau label encoder avec les classes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S95GM7craq6U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_emotion_avatars(emotions_path):\n",
    "    \"\"\"\n",
    "    Charge les images des avatars correspondant aux émotions.\n",
    "    \n",
    "    Parameters:\n",
    "        emotions_path (str): Chemin vers le dossier contenant les images des avatars.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionnaire associant chaque émotion à son image.\n",
    "    \"\"\"\n",
    "    emotion_files = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    emotion_to_emoji = {}\n",
    "    for emotion in emotion_files:\n",
    "        image_path = os.path.join(emotions_path, f\"{emotion}.png\")\n",
    "        if os.path.exists(image_path):\n",
    "            emotion_to_emoji[emotion] = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Lire avec la transparence\n",
    "        else:\n",
    "            print(f\"Image manquante pour l'émotion: {emotion}\")\n",
    "    return emotion_to_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_avatar(emotion, emotion_to_emoji):\n",
    "    \"\"\"\n",
    "    Récupère l'image de l'avatar correspondant à une émotion donnée.\n",
    "    \n",
    "    Parameters:\n",
    "        emotion (str): Le label de l'émotion détectée.\n",
    "        emotion_to_emoji (dict): Dictionnaire associant chaque émotion à son image.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Image de l'avatar correspondant à l'émotion ou None si non trouvé.\n",
    "    \"\"\"\n",
    "    return emotion_to_emoji.get(emotion, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_features(face, cnn_encoder, face_landmarks_func):\n",
    "    \"\"\"\n",
    "    Extrait les descripteurs faciaux à l'aide du modèle CNN de Dlib.\n",
    "    \"\"\"\n",
    "    # Extraire les repères faciaux à partir de l'image du visage\n",
    "    landmarks = face_landmarks_func(face)\n",
    "\n",
    "    # Calculer les descripteurs faciaux\n",
    "    features = np.array(cnn_encoder.compute_face_descriptor(face, landmarks))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from fer import FER\n",
    "import joblib\n",
    "\n",
    "def live_webcam_interpret(model, emotion_to_emoji, label_encoder=None):\n",
    "    \"\"\"\n",
    "    Capture la vidéo en direct depuis la webcam et prédit les émotions en temps réel.\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle KNN entraîné sur les vecteurs émotionnels (7 dimensions).\n",
    "        emotion_to_emoji: Dictionnaire mappant les émotions aux images d'emojis.\n",
    "        label_encoder: LabelEncoder utilisé pour encoder/décoder les étiquettes d'émotion.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # Activer la webcam par défaut\n",
    "    detector = FER(mtcnn=True)  # Initialiser FER pour la détection d'émotions\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Échec de la capture de la vidéo\")\n",
    "            break\n",
    "        \n",
    "        # Convertir le cadre en RGB (car FER utilise RGB)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Détection des émotions\n",
    "        emotion_data = detector.detect_emotions(rgb_frame)\n",
    "        \n",
    "        if emotion_data:  # Vérifier si des émotions sont détectées\n",
    "            emotions = emotion_data[0]['emotions']  # Prendre le premier visage détecté\n",
    "            emotion_vector = list(emotions.values())  # Extraire le vecteur d'émotions (7 valeurs)\n",
    "            \n",
    "            # Prédiction avec le modèle KNN\n",
    "            emotion_index = model.predict([emotion_vector])[0]\n",
    "            emotion_label = label_encoder.inverse_transform([emotion_index])[0] if label_encoder else emotion_index\n",
    "            \n",
    "            # Charger et afficher l'emoji correspondant\n",
    "            emoji_image = emotion_to_emoji.get(emotion_label, None)\n",
    "            # Vérifier si l'emoji a un canal alpha (transparence)\n",
    "            if emoji_image is not None:\n",
    "                # Redimensionner l'emoji à une taille plus petite\n",
    "                emoji_resized = cv2.resize(emoji_image, (50, 50))\n",
    "            \n",
    "                \n",
    "                if emoji_resized.shape[2] == 4:\n",
    "                    # Convertir RGBA en BGR (en ignorant le canal alpha)\n",
    "                    emoji_resized = cv2.cvtColor(emoji_resized, cv2.COLOR_RGBA2BGR)\n",
    "                    emoji_resized = cv2.cvtColor(emoji_resized, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "                # Ajouter l'emoji sur l'image\n",
    "                x_offset, y_offset = 10, 10  # Coordonnées pour placer l'emoji\n",
    "                frame[y_offset:y_offset + emoji_resized.shape[0], x_offset:x_offset + emoji_resized.shape[1]] = emoji_resized\n",
    "        \n",
    "            # Ajouter l'emoji sur l'image\n",
    "            x_offset, y_offset = 10, 10  # Coordonnées pour placer l'emoji\n",
    "            frame[y_offset:y_offset + emoji_resized.shape[0], x_offset:x_offset + emoji_resized.shape[1]] = emoji_resized\n",
    "            \n",
    "            # Ajouter un texte d'émotion au-dessus de l'emoji\n",
    "            cv2.putText(frame, emotion_label, (x_offset, y_offset + 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        text = \"Press 'q' to Quit\"\n",
    "        cv2.putText(frame, text, (x_offset, frame_height - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Afficher la vidéo en direct\n",
    "        cv2.imshow(\"Emotion Prediction with Webcam\", frame)\n",
    "        \n",
    "        # Arrêter la boucle si 'q' est pressé\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Libérer la capture et fermer les fenêtres\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "grid_search_knn = joblib.load('grid_search_knn.pkl')\n",
    "label_classes = np.load('label_classes.npy')  # Charger les classes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = label_classes\n",
    "\n",
    "# Charger les avatars des émotions\n",
    "emotions_path = \"emotions\"  # Dossier contenant les images PNG des avatars\n",
    "emotion_to_emoji = load_emotion_avatars(emotions_path)\n",
    "\n",
    "# Tester la fonction\n",
    "live_webcam_interpret(grid_search_knn, emotion_to_emoji, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
